# Dependencies
import streamlit as st
import mdptoolbox
import mdptoolbox.example
import pandas as pd
import numpy as np
from inform import Descriptions
from model_dependencies import mdp_dependencies

def solver():

    """"
    solver() runs the MDP and display 
    MDP page components. It uses functions defined
    after such that this function could be simplified.
    """

    st.title("Markov-Decision Process Solver")
    st.markdown('---')

    c1, c2 = st.columns((2, 1))
    c1.header('Input')

    c2.header('Description')
    c2.info(Descriptions.MDP_ABOUT)
    c2.error(Descriptions.MDP_INPUT)
    c2.success(Descriptions.MDP_OUTPUT)

    # Option to decide whether or not to use data generated by us
    data_options = ['Import own data', 'Use data collected by authors']
    option = c1.radio('Which data would you like the model to consider?', data_options)
    
    if (option == data_options[0]):

        # Own Data
        rewards = c1.file_uploader("Upload Rewards Dataframe", type=["csv"], key='reward_mdp')
        transitions = c1.file_uploader("Upload Transition Probability Dataframe", type=["csv"], key='transitions_mdp')

        if (rewards is not None and transitions is not None):
            data_rewards = pd.read_csv(rewards).iloc[: , 1:]
            data_transitions= pd.read_csv(transitions).iloc[: , 1:]

            # How to solve the model
            solver_options = ['Policy Iteration', 'Value Iteration', 'Q-Learnings']
            solver_chosen = c1.selectbox("How should the problem be solved?", solver_options, help = Descriptions.SOLVERS)

            if (solver_chosen == "Q-Learnings"):
                number_iterations = st.number_input("Select max number of iteration for the Q-Learnings", min_value = 10000, help="E.g. 20.000 Iterations. Hint: The minimum number is 10.000!", step = 1)
            else: 
                number_iterations = 0.0
            
            discount_factor = get_discount_factor(c1, solver_chosen)
                
            display_data(data_rewards, data_transitions)

            reward_matrix, number_actions, number_states = input_to_reward_matrix(data_rewards)
            probability_matrix = input_to_probability_matrix(data_transitions, number_actions, number_states)

            result_dict = mdp_dependencies.solve_markov_decision_process(probability_matrix, reward_matrix, discount_factor, solver_chosen, number_iterations)
            optimal_policy = pd.DataFrame(list(result_dict.get("Optimal Policy")), columns = ['action_category']) 

            transition_action_map = dict(zip(data_transitions['action_category'], data_transitions['action']))

            # st.write(transition_action_map) 
            optimal_policy['action'] = optimal_policy['action_category'].map(transition_action_map)

            st.write(optimal_policy)  
            # csv = convert_df(optimal_policy)

            rewards_list = data_rewards['Reward (state, action, follow_up_state)'].to_list()
            action_cost_list = data_rewards['cost'].to_list()
            data_transitions['Reward (state, action, follow_up_state)'] = rewards_list
            data_transitions['cost'] = action_cost_list
            # st.write(data_transitions)

            st.download_button(
                "Download Optimal Policy",
                convert_df(optimal_policy),
                "mcp_optimal_policy.csv",
                "text/csv",
                key='optimal-csv'
            )
            st.download_button(
                "Download MCP Input",
                convert_df(data_transitions),
                "mcp_input.csv",
                "text/csv",
                key='mcp-csv'
            )

        else:
            st.markdown('---')
            st.warning('Before we start, you need to feed the algorithm some data!')

    else: 

        # Own Data
        data_rewards = pd.read_csv('data/datasets/official/markov_decision_process/mdp_rewards.csv')
        data_transitions = pd.read_csv('data/datasets/official/markov_decision_process/mdp_transitions.csv')

        # How to solve the model
        solver_options = ['Policy Iteration', 'Value Iteration', 'Q-Learnings']
        solver_chosen = c1.selectbox("How should the problem be solved?", solver_options, help = Descriptions.SOLVERS)

        if (solver_chosen == "Q-Learnings"):
            number_iterations = st.number_input("Select max number of iteration for the Q-Learnings", min_value = 10000, help="E.g. 20.000 Iterations. Hint: The minimum number is 10.000!", step = 1)
        else: 
            number_iterations = 0.0
        
        discount_factor = get_discount_factor(c1, solver_chosen)
            
        display_data(data_rewards, data_transitions)

        reward_matrix, number_actions, number_states = input_to_reward_matrix(data_rewards)
        probability_matrix = input_to_probability_matrix(data_transitions, number_actions, number_states)

        result_dict = mdp_dependencies.solve_markov_decision_process(probability_matrix, reward_matrix, discount_factor, solver_chosen, number_iterations)
        optimal_policy = pd.DataFrame(list(result_dict.get("Optimal Policy")), columns = ['action_category']) 

        transition_action_map = dict(zip(data_transitions['action_category'], data_transitions['action']))

        # st.write(transition_action_map) 
        optimal_policy['action'] = optimal_policy['action_category'].map(transition_action_map)

        st.write(optimal_policy)  
        # csv = convert_df(optimal_policy)

        rewards_list = data_rewards['Reward (state, action, follow_up_state)'].to_list()
        action_cost_list = data_rewards['cost'].to_list()
        data_transitions['Reward (state, action, follow_up_state)'] = rewards_list
        data_transitions['cost'] = action_cost_list
        # st.write(data_transitions)

        st.download_button(
            "Download Optimal Policy",
            convert_df(optimal_policy),
            "mcp_optimal_policy.csv",
            "text/csv",
            key='optimal-csv'
        )
        st.download_button(
            "Download MCP Input",
            convert_df(data_transitions),
            "mcp_input.csv",
            "text/csv",
            key='mcp-csv'
        )
    
def input_to_reward_matrix(data):

    """
    input_to_reward_matrix(data)
    transforms dataframe of rewards into a list of matrices with rewards for every 
    specific action.

    :param data: Rewards Dataframe

    :return reward_matrices: list of reward matrices
    """

    st.markdown('---')
    st.markdown('## Input Transformation: Rewards')

    # Action Count
    action_count = data["action_category"].value_counts(normalize=True)
    number_actions = len(action_count)

    # State Count
    state_count = data["state_category"].value_counts(normalize=True)
    number_states = len(state_count)

    c1, c2 = st.columns(2)

    c1.markdown("#### Reward Matrices (S,A,S')")

    # REWARD Matrix
    reward_matrices = []

    for i in range(number_actions):

        action_reward_matrix = np.zeros((number_states,number_states))

        for j in range(number_states):
            for h in range (number_states):

                coordinate = (j, i, h)

                for l in range (len(data.index)):
                    cooperator = (data['state_category'][l], data['action_category'][l], data['follow_up_state_category'][l])

                    if (cooperator[0] == coordinate[0] and cooperator[1] == coordinate[1] and cooperator[2] == coordinate[2]):
                        action_reward_matrix[j][h] = data['Reward (state, action, follow_up_state)'][l]

                        # st.write('Constructed ({})'.format(cooperator))
    
            
        c1.write('Action {} Matrix'.format(i))
        c1.write(action_reward_matrix)

        reward_matrices.append(action_reward_matrix)

    c2.markdown('#### Storage')
    c2.write(reward_matrices)

    return reward_matrices, number_actions, number_states

def input_to_probability_matrix(data, number_actions, number_states):

    """
    input_to_probability_matrix(data, number_actions, number_states)
    transforms dataframe of trans. prob. into a list of matrices for every 
    specific action.

    :param data: Trans. Prob. Dataframe
    :param number_actions: Number Actions
    :param number_states: Number States

    :return transition_matrices: list of probability matrices
    """

    st.markdown('---')
    st.markdown('## Input Transformation: Transition Probability')

    c1, c2 = st.columns(2)

    c1.markdown("#### Transition Matrices (S,A,S')")

    # REWARD Matrix
    transition_matrices = []

    for i in range(number_actions):

        action_transition_matrix = np.zeros((number_states,number_states))

        for j in range(number_states):
            for h in range (number_states):

                coordinate = (j, i, h)

                for l in range (len(data.index)):
                    cooperator = (data['state_category'][l], data['action_category'][l], data['follow_up_state_category'][l])

                    if (cooperator[0] == coordinate[0] and cooperator[1] == coordinate[1] and cooperator[2] == coordinate[2]):
                        action_transition_matrix[j][h] = data['Probability Triple'][l]

                        # st.write('Constructed ({})'.format(cooperator))
    
            
        c1.write('Action {} Matrix'.format(i))
        c1.write(action_transition_matrix)

        transition_matrices.append(action_transition_matrix)

    c2.markdown('#### Storage')
    c2.write(transition_matrices)

    return transition_matrices

def display_data(rewards, transitions):

    """
    display_data(...) visualizes the inputs of
    the MDP sections (Rewards and Transitions)
    """

    st.markdown('---')
    st.markdown('## Display Inputs')

    c1, c2 = st.columns(2)
    c1.header('Rewards')
    c1.write(rewards)

    c2.header('Transition Probabilities')
    c2.write(transitions)

def get_discount_factor(c1, mdp_solver):

    """
    get_discount_factor(...)

    :param c1: Streamlit Column
    :param mdp_solver: {Policy Iteration, Value Iteration or Q-Learning}

    :return discounting factor
    """

    if (mdp_solver == "Value Iteration"):
        discount_factor = 1.0
   
    else:
        # Input Discount Factor
        wacc = c1.slider("WACC Factor", min_value=0.01, max_value = 1.0, value = 0.07, help="Default WACC value set to 7%.")
        periods = int(c1.number_input("Number of decision periods in 1 year", min_value = 2, value = 12, step = 1))
        discount_factor = np.power(1/(1+wacc), 1/periods)
        # c1.write('Discount Factor is {}'.format(discount_factor))
    
    return discount_factor

def select_user_journey():

    """
    select_user_journey() is responsable for
    allowing the user to sleect his UX by either drag-and-drop 
    path or the already calculated experience. 

    :return data_rewards: MDP Rewards
    :return data_transitions: MDP Trans. Prob.
    :return discount_factor: MDP Discounting Factor
    :return solver_chosen: MDP Solver Choice
    """

    c1, c2 = st.columns((2, 1))
    c1.header('Input')

    c2.header('Description')
    c2.info(Descriptions.MDP_ABOUT)
    c2.error(Descriptions.MDP_INPUT)
    c2.success(Descriptions.MDP_OUTPUT)

    # Option to decide whether or not to use data generated by us
    data_options = ['Import own data', 'Use data collected by authors']
    option = c1.radio('Which data would you like the model to consider?', data_options)
    
    if (option == data_options[0]):

        # Own Data
        rewards = c1.file_uploader("Upload Rewards Dataframe", type=["csv"], key='reward_mdp')
        transitions = c1.file_uploader("Upload Transition Probability Dataframe", type=["csv"], key='transitions_mdp')

        if (rewards is not None and transitions is not None):
            data_rewards = pd.read_csv(rewards).iloc[: , 1:]
            data_transitions= pd.read_csv(transitions).iloc[: , 1:]

            # How to solve the model
            solver_options = ['Policy Iteration', 'Value Iteration', 'Q-Learnings']
            solver_chosen = c1.selectbox("How should the problem be solved?", solver_options, help = Descriptions.SOLVERS)

            if (solver_chosen == "Q-Learnings"):
                number_iterations = st.number_input("Select max number of iteration for the Q-Learnings", min_value = 10000, help="E.g. 20.000 Iterations. Hint: The minimum number is 10.000!", step = 1)
            else: 
                number_iterations = 0.0
            
            discount_factor = get_discount_factor(c1, solver_chosen)
                
            display_data(rewards, transitions)

            reward_matrix, number_actions, number_states = input_to_reward_matrix(rewards)
            probability_matrix = input_to_probability_matrix(transitions, number_actions, number_states)

            result_dict = mdp_dependencies.solve_markov_decision_process(probability_matrix, reward_matrix, discount_factor, mdp_solver, number_iterations)
            optimal_policy = pd.DataFrame(list(result_dict.get("Optimal Policy")), columns = ['action_category']) 

            transition_action_map = dict(zip(transitions['action_category'], transitions['action']))

            # st.write(transition_action_map) 
            optimal_policy['action'] = optimal_policy['action_category'].map(transition_action_map)

            st.write(optimal_policy)  

            csv = convert_df(optimal_policy)

            st.download_button(
                "Press to Download",
                csv,
                "optimal_policy.csv",
                "text/csv",
                key='optimal-csv'
            )

    else:

        # Input Data
        data_rewards = pd.read_csv("data/datasets/official/markov_decision_process/rewards_mdp.csv")
        data_transitions = pd.read_csv("data/datasets/official/markov_decision_process/rewards_mdp.csv")

         # Input Discount Factor
        discount_factor = c1.slider("Discount Factor", min_value=0.01, 
                                                    max_value = 1.0,
                                                    value = 0.07, 
                                                    help="Default discount value measured by WACC at 7%.")

        # How to solve the model
        solver_options = ['Policy Iteration', 'Value Iteration', 'Q-Learnings']
        solver_chosen = c1.selectbox("How should the problem be solved?", solver_options, help = Descriptions.SOLVERS)
        return data_rewards, data_transitions, discount_factor, solver_chosen


@st.cache
def convert_df(df):
   """
   convert_df(...) returns dataframe converted
   into .csv file
   """
   return df.to_csv().encode('utf-8')